{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292a0f8e-aa4f-41d8-b093-8e6c2e94e198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if using colab, do !pip install torch_geometric before running this file\n",
    "from torch_geometric.datasets import Amazon\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548609c5-9003-4511-be46-bb05331a1fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.32471340975739804\n",
      "Epoch: 000, Loss: 2.35122, Train Acc: 0.33050, Test Acc: 0.32471\n",
      "Test accuracy 0.6001066382298054\n",
      "Epoch: 001, Loss: 2.21840, Train Acc: 0.60290, Test Acc: 0.60011\n",
      "Test accuracy 0.6419621434284191\n",
      "Epoch: 002, Loss: 2.08420, Train Acc: 0.65110, Test Acc: 0.64196\n",
      "Epoch: 003, Loss: 1.91961, Train Acc: 0.64400, Test Acc: 0.63583\n",
      "Epoch: 004, Loss: 1.73305, Train Acc: 0.64920, Test Acc: 0.64116\n",
      "Test accuracy 0.6598240469208211\n",
      "Epoch: 005, Loss: 1.53655, Train Acc: 0.66690, Test Acc: 0.65982\n",
      "Test accuracy 0.7030125299920021\n",
      "Epoch: 006, Loss: 1.35265, Train Acc: 0.70930, Test Acc: 0.70301\n",
      "Test accuracy 0.7280725139962677\n",
      "Epoch: 007, Loss: 1.18455, Train Acc: 0.73510, Test Acc: 0.72807\n",
      "Test accuracy 0.7422020794454812\n",
      "Epoch: 008, Loss: 1.04558, Train Acc: 0.75300, Test Acc: 0.74220\n",
      "Test accuracy 0.7515329245534524\n",
      "Epoch: 009, Loss: 0.93804, Train Acc: 0.76180, Test Acc: 0.75153\n",
      "Test accuracy 0.7632631298320448\n",
      "Epoch: 010, Loss: 0.84329, Train Acc: 0.77340, Test Acc: 0.76326\n",
      "Test accuracy 0.7885897094108237\n",
      "Epoch: 011, Loss: 0.77671, Train Acc: 0.79220, Test Acc: 0.78859\n",
      "Test accuracy 0.7987203412423354\n",
      "Epoch: 012, Loss: 0.71504, Train Acc: 0.81250, Test Acc: 0.79872\n",
      "Test accuracy 0.8080511863503066\n",
      "Epoch: 013, Loss: 0.67237, Train Acc: 0.82230, Test Acc: 0.80805\n",
      "Test accuracy 0.8133830978405758\n",
      "Epoch: 014, Loss: 0.62158, Train Acc: 0.82570, Test Acc: 0.81338\n",
      "Test accuracy 0.8237803252466009\n",
      "Epoch: 015, Loss: 0.58278, Train Acc: 0.83980, Test Acc: 0.82378\n",
      "Test accuracy 0.8467075446547587\n",
      "Epoch: 016, Loss: 0.54498, Train Acc: 0.86010, Test Acc: 0.84671\n",
      "Test accuracy 0.8541722207411357\n",
      "Epoch: 017, Loss: 0.51233, Train Acc: 0.86960, Test Acc: 0.85417\n",
      "Test accuracy 0.8603039189549454\n",
      "Epoch: 018, Loss: 0.49185, Train Acc: 0.87480, Test Acc: 0.86030\n",
      "Test accuracy 0.8675019994668088\n",
      "Epoch: 019, Loss: 0.46023, Train Acc: 0.88120, Test Acc: 0.86750\n",
      "Test accuracy 0.8720341242335378\n",
      "Epoch: 020, Loss: 0.44748, Train Acc: 0.88550, Test Acc: 0.87203\n",
      "Test accuracy 0.8786990135963743\n",
      "Epoch: 021, Loss: 0.42323, Train Acc: 0.89370, Test Acc: 0.87870\n",
      "Test accuracy 0.8869634764062917\n",
      "Epoch: 022, Loss: 0.41378, Train Acc: 0.90160, Test Acc: 0.88696\n",
      "Test accuracy 0.8938949613436417\n",
      "Epoch: 023, Loss: 0.39377, Train Acc: 0.90700, Test Acc: 0.89389\n",
      "Test accuracy 0.895227939216209\n",
      "Epoch: 024, Loss: 0.38531, Train Acc: 0.91080, Test Acc: 0.89523\n",
      "Test accuracy 0.895761130365236\n",
      "Epoch: 025, Loss: 0.37473, Train Acc: 0.91170, Test Acc: 0.89576\n",
      "Epoch: 026, Loss: 0.36361, Train Acc: 0.91230, Test Acc: 0.89336\n",
      "Epoch: 027, Loss: 0.35205, Train Acc: 0.91160, Test Acc: 0.89496\n",
      "Test accuracy 0.8965609170887763\n",
      "Epoch: 028, Loss: 0.34780, Train Acc: 0.91230, Test Acc: 0.89656\n",
      "Test accuracy 0.8997600639829378\n",
      "Epoch: 029, Loss: 0.33443, Train Acc: 0.91610, Test Acc: 0.89976\n",
      "Test accuracy 0.9013596374300187\n",
      "Epoch: 030, Loss: 0.32077, Train Acc: 0.91790, Test Acc: 0.90136\n",
      "Epoch: 031, Loss: 0.32242, Train Acc: 0.91920, Test Acc: 0.90109\n",
      "Epoch: 032, Loss: 0.30716, Train Acc: 0.92260, Test Acc: 0.90003\n",
      "Epoch: 033, Loss: 0.30264, Train Acc: 0.92090, Test Acc: 0.90029\n",
      "Test accuracy 0.9018928285790456\n",
      "Epoch: 034, Loss: 0.29834, Train Acc: 0.92250, Test Acc: 0.90189\n",
      "Test accuracy 0.9026926153025859\n",
      "Epoch: 035, Loss: 0.29437, Train Acc: 0.92500, Test Acc: 0.90269\n",
      "Test accuracy 0.9042921887496668\n",
      "Epoch: 036, Loss: 0.28735, Train Acc: 0.92400, Test Acc: 0.90429\n",
      "Epoch: 037, Loss: 0.28390, Train Acc: 0.92610, Test Acc: 0.90403\n",
      "Test accuracy 0.9048253798986937\n",
      "Epoch: 038, Loss: 0.28249, Train Acc: 0.92790, Test Acc: 0.90483\n",
      "Test accuracy 0.906158357771261\n",
      "Epoch: 039, Loss: 0.27253, Train Acc: 0.92830, Test Acc: 0.90616\n",
      "Epoch: 040, Loss: 0.26978, Train Acc: 0.92900, Test Acc: 0.90563\n",
      "Test accuracy 0.9066915489202879\n",
      "Epoch: 041, Loss: 0.26888, Train Acc: 0.92950, Test Acc: 0.90669\n",
      "Test accuracy 0.9074913356438283\n",
      "Epoch: 042, Loss: 0.26623, Train Acc: 0.92920, Test Acc: 0.90749\n",
      "Test accuracy 0.9077579312183418\n",
      "Epoch: 043, Loss: 0.26477, Train Acc: 0.92950, Test Acc: 0.90776\n",
      "Test accuracy 0.9082911223673688\n",
      "Epoch: 044, Loss: 0.25808, Train Acc: 0.93110, Test Acc: 0.90829\n",
      "Test accuracy 0.9093575046654225\n",
      "Epoch: 045, Loss: 0.25443, Train Acc: 0.93270, Test Acc: 0.90936\n",
      "Test accuracy 0.910157291388963\n",
      "Epoch: 046, Loss: 0.25097, Train Acc: 0.93320, Test Acc: 0.91016\n",
      "Test accuracy 0.9106904825379899\n",
      "Epoch: 047, Loss: 0.25085, Train Acc: 0.93250, Test Acc: 0.91069\n",
      "Epoch: 048, Loss: 0.24618, Train Acc: 0.93270, Test Acc: 0.91042\n",
      "Test accuracy 0.9112236736870168\n",
      "Epoch: 049, Loss: 0.24934, Train Acc: 0.93350, Test Acc: 0.91122\n",
      "Epoch: 050, Loss: 0.24897, Train Acc: 0.93420, Test Acc: 0.91096\n",
      "Epoch: 051, Loss: 0.24371, Train Acc: 0.93470, Test Acc: 0.91042\n",
      "Test accuracy 0.9117568648360437\n",
      "Epoch: 052, Loss: 0.23983, Train Acc: 0.93460, Test Acc: 0.91176\n",
      "Epoch: 053, Loss: 0.24372, Train Acc: 0.93490, Test Acc: 0.91149\n",
      "Test accuracy 0.9120234604105572\n",
      "Epoch: 054, Loss: 0.23856, Train Acc: 0.93510, Test Acc: 0.91202\n",
      "Test accuracy 0.9125566515595841\n",
      "Epoch: 055, Loss: 0.23859, Train Acc: 0.93550, Test Acc: 0.91256\n",
      "Epoch: 056, Loss: 0.23583, Train Acc: 0.93580, Test Acc: 0.91202\n",
      "Epoch: 057, Loss: 0.23685, Train Acc: 0.93700, Test Acc: 0.91096\n",
      "Epoch: 058, Loss: 0.23346, Train Acc: 0.93690, Test Acc: 0.91176\n",
      "Epoch: 059, Loss: 0.23222, Train Acc: 0.93730, Test Acc: 0.91256\n",
      "Test accuracy 0.9133564382831245\n",
      "Epoch: 060, Loss: 0.23471, Train Acc: 0.93760, Test Acc: 0.91336\n",
      "Test accuracy 0.913623033857638\n",
      "Epoch: 061, Loss: 0.23156, Train Acc: 0.93740, Test Acc: 0.91362\n",
      "Epoch: 062, Loss: 0.23115, Train Acc: 0.93780, Test Acc: 0.91309\n",
      "Epoch: 063, Loss: 0.23039, Train Acc: 0.93790, Test Acc: 0.91176\n",
      "Epoch: 064, Loss: 0.23029, Train Acc: 0.93850, Test Acc: 0.91336\n",
      "Test accuracy 0.9138896294321515\n",
      "Epoch: 065, Loss: 0.22761, Train Acc: 0.93840, Test Acc: 0.91389\n",
      "Epoch: 066, Loss: 0.22916, Train Acc: 0.93950, Test Acc: 0.91389\n",
      "Test accuracy 0.9157557984537457\n",
      "Epoch: 067, Loss: 0.22665, Train Acc: 0.94000, Test Acc: 0.91576\n",
      "Epoch: 068, Loss: 0.22550, Train Acc: 0.93980, Test Acc: 0.91576\n",
      "Epoch: 069, Loss: 0.22130, Train Acc: 0.93930, Test Acc: 0.91576\n",
      "Epoch: 070, Loss: 0.22515, Train Acc: 0.93880, Test Acc: 0.91522\n",
      "Epoch: 071, Loss: 0.22130, Train Acc: 0.93920, Test Acc: 0.91522\n",
      "Test accuracy 0.9160223940282591\n",
      "Epoch: 072, Loss: 0.22287, Train Acc: 0.94090, Test Acc: 0.91602\n",
      "Test accuracy 0.9162889896027726\n",
      "Epoch: 073, Loss: 0.22040, Train Acc: 0.94090, Test Acc: 0.91629\n",
      "Epoch: 074, Loss: 0.22364, Train Acc: 0.94100, Test Acc: 0.91629\n",
      "Epoch: 075, Loss: 0.21956, Train Acc: 0.94040, Test Acc: 0.91629\n",
      "Epoch: 076, Loss: 0.21909, Train Acc: 0.94010, Test Acc: 0.91522\n",
      "Epoch: 077, Loss: 0.22112, Train Acc: 0.94060, Test Acc: 0.91549\n",
      "Epoch: 078, Loss: 0.22000, Train Acc: 0.94020, Test Acc: 0.91549\n",
      "Epoch: 079, Loss: 0.21675, Train Acc: 0.94040, Test Acc: 0.91602\n",
      "Test accuracy 0.9165555851772861\n",
      "Epoch: 080, Loss: 0.22123, Train Acc: 0.94160, Test Acc: 0.91656\n",
      "Test accuracy 0.9168221807517996\n",
      "Epoch: 081, Loss: 0.21740, Train Acc: 0.94240, Test Acc: 0.91682\n",
      "Test accuracy 0.9173553719008265\n",
      "Epoch: 082, Loss: 0.21719, Train Acc: 0.94200, Test Acc: 0.91736\n",
      "Epoch: 083, Loss: 0.21418, Train Acc: 0.94210, Test Acc: 0.91736\n",
      "Epoch: 084, Loss: 0.21458, Train Acc: 0.94150, Test Acc: 0.91549\n",
      "Epoch: 085, Loss: 0.21511, Train Acc: 0.94250, Test Acc: 0.91709\n",
      "Epoch: 086, Loss: 0.21253, Train Acc: 0.94330, Test Acc: 0.91682\n",
      "Epoch: 087, Loss: 0.21217, Train Acc: 0.94350, Test Acc: 0.91682\n",
      "Epoch: 088, Loss: 0.21406, Train Acc: 0.94300, Test Acc: 0.91629\n",
      "Epoch: 089, Loss: 0.21558, Train Acc: 0.94260, Test Acc: 0.91709\n",
      "Epoch: 090, Loss: 0.21249, Train Acc: 0.94170, Test Acc: 0.91549\n",
      "Epoch: 091, Loss: 0.21392, Train Acc: 0.94160, Test Acc: 0.91576\n",
      "Test accuracy 0.9176219674753399\n",
      "Epoch: 092, Loss: 0.21024, Train Acc: 0.94350, Test Acc: 0.91762\n",
      "Test accuracy 0.9178885630498533\n",
      "Epoch: 093, Loss: 0.21186, Train Acc: 0.94410, Test Acc: 0.91789\n",
      "Epoch: 094, Loss: 0.20842, Train Acc: 0.94420, Test Acc: 0.91762\n",
      "Epoch: 095, Loss: 0.21018, Train Acc: 0.94390, Test Acc: 0.91629\n",
      "Epoch: 096, Loss: 0.20798, Train Acc: 0.94310, Test Acc: 0.91629\n",
      "Epoch: 097, Loss: 0.20706, Train Acc: 0.94330, Test Acc: 0.91682\n",
      "Epoch: 098, Loss: 0.20765, Train Acc: 0.94270, Test Acc: 0.91576\n",
      "Epoch: 099, Loss: 0.21200, Train Acc: 0.94450, Test Acc: 0.91709\n",
      "Epoch: 100, Loss: 0.20537, Train Acc: 0.94450, Test Acc: 0.91629\n",
      "Epoch: 101, Loss: 0.20942, Train Acc: 0.94430, Test Acc: 0.91682\n",
      "Epoch: 102, Loss: 0.20266, Train Acc: 0.94450, Test Acc: 0.91522\n",
      "Epoch: 103, Loss: 0.20286, Train Acc: 0.94560, Test Acc: 0.91522\n",
      "Epoch: 104, Loss: 0.20690, Train Acc: 0.94540, Test Acc: 0.91789\n",
      "Test accuracy 0.9181551586243668\n",
      "Epoch: 105, Loss: 0.19985, Train Acc: 0.94550, Test Acc: 0.91816\n",
      "Epoch: 106, Loss: 0.20323, Train Acc: 0.94610, Test Acc: 0.91816\n",
      "Test accuracy 0.9192215409224207\n",
      "Epoch: 107, Loss: 0.20590, Train Acc: 0.94520, Test Acc: 0.91922\n",
      "Epoch: 108, Loss: 0.20076, Train Acc: 0.94370, Test Acc: 0.91842\n",
      "Epoch: 109, Loss: 0.19776, Train Acc: 0.94400, Test Acc: 0.91816\n",
      "Epoch: 110, Loss: 0.20237, Train Acc: 0.94490, Test Acc: 0.91762\n",
      "Epoch: 111, Loss: 0.20348, Train Acc: 0.94550, Test Acc: 0.91709\n",
      "Epoch: 112, Loss: 0.20152, Train Acc: 0.94630, Test Acc: 0.91816\n",
      "Epoch: 113, Loss: 0.20270, Train Acc: 0.94490, Test Acc: 0.91576\n",
      "Epoch: 114, Loss: 0.19903, Train Acc: 0.94640, Test Acc: 0.91922\n",
      "Epoch: 115, Loss: 0.19901, Train Acc: 0.94630, Test Acc: 0.91789\n",
      "Epoch: 116, Loss: 0.19602, Train Acc: 0.94620, Test Acc: 0.91895\n",
      "Epoch: 117, Loss: 0.20144, Train Acc: 0.94570, Test Acc: 0.91842\n",
      "Epoch: 118, Loss: 0.19691, Train Acc: 0.94640, Test Acc: 0.91789\n",
      "Epoch: 119, Loss: 0.19690, Train Acc: 0.94660, Test Acc: 0.91682\n",
      "Epoch: 120, Loss: 0.20005, Train Acc: 0.94570, Test Acc: 0.91602\n",
      "Epoch: 121, Loss: 0.19746, Train Acc: 0.94480, Test Acc: 0.91629\n",
      "Epoch: 122, Loss: 0.19686, Train Acc: 0.94630, Test Acc: 0.91789\n",
      "Epoch: 123, Loss: 0.19360, Train Acc: 0.94690, Test Acc: 0.91816\n",
      "Epoch: 124, Loss: 0.19776, Train Acc: 0.94710, Test Acc: 0.91869\n",
      "Epoch: 125, Loss: 0.19653, Train Acc: 0.94680, Test Acc: 0.91842\n",
      "Epoch: 126, Loss: 0.19352, Train Acc: 0.94680, Test Acc: 0.91816\n",
      "Epoch: 127, Loss: 0.19604, Train Acc: 0.94660, Test Acc: 0.91709\n",
      "Epoch: 128, Loss: 0.19414, Train Acc: 0.94610, Test Acc: 0.91709\n",
      "Epoch: 129, Loss: 0.19365, Train Acc: 0.94750, Test Acc: 0.91682\n",
      "Epoch: 130, Loss: 0.19536, Train Acc: 0.94650, Test Acc: 0.91789\n",
      "Epoch: 131, Loss: 0.19142, Train Acc: 0.94630, Test Acc: 0.91682\n",
      "Epoch: 132, Loss: 0.19409, Train Acc: 0.94650, Test Acc: 0.91816\n",
      "Epoch: 133, Loss: 0.19267, Train Acc: 0.94680, Test Acc: 0.91789\n",
      "Epoch: 134, Loss: 0.19458, Train Acc: 0.94780, Test Acc: 0.91762\n",
      "Epoch: 135, Loss: 0.19330, Train Acc: 0.94700, Test Acc: 0.91629\n",
      "Epoch: 136, Loss: 0.19238, Train Acc: 0.94960, Test Acc: 0.91869\n",
      "Epoch: 137, Loss: 0.19057, Train Acc: 0.94890, Test Acc: 0.91895\n",
      "Epoch: 138, Loss: 0.18988, Train Acc: 0.94700, Test Acc: 0.91789\n",
      "Epoch: 139, Loss: 0.19277, Train Acc: 0.94450, Test Acc: 0.91762\n",
      "Test accuracy 0.9202879232204746\n",
      "Epoch: 140, Loss: 0.19461, Train Acc: 0.94670, Test Acc: 0.92029\n",
      "Epoch: 141, Loss: 0.19239, Train Acc: 0.94910, Test Acc: 0.92029\n",
      "Epoch: 142, Loss: 0.19422, Train Acc: 0.94900, Test Acc: 0.91762\n",
      "Epoch: 143, Loss: 0.19108, Train Acc: 0.94550, Test Acc: 0.91682\n",
      "Epoch: 144, Loss: 0.19241, Train Acc: 0.94640, Test Acc: 0.91762\n",
      "Test accuracy 0.9208211143695014\n",
      "Epoch: 145, Loss: 0.19167, Train Acc: 0.94920, Test Acc: 0.92082\n",
      "Epoch: 146, Loss: 0.19399, Train Acc: 0.94890, Test Acc: 0.91922\n",
      "Epoch: 147, Loss: 0.19096, Train Acc: 0.94600, Test Acc: 0.91522\n",
      "Epoch: 148, Loss: 0.19323, Train Acc: 0.94570, Test Acc: 0.91869\n",
      "Epoch: 149, Loss: 0.18640, Train Acc: 0.94770, Test Acc: 0.91736\n"
     ]
    }
   ],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Step 1: Add self-loops\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Multiply with weights\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3: Calculate the normalization\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4: Propagate the embeddings to the next layer\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x,\n",
    "                              norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset, n_in):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(n_in, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        self.out = torch.nn.Linear(64, dataset.num_classes)\n",
    "        self.hook = self.conv2.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.intermediate_output = output\n",
    "\n",
    "    def forward(self, data, source_features_reduced):\n",
    "        x, edge_index = source_features_reduced, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def plot_dataset(dataset):\n",
    "    edges_raw = dataset.data.edge_index.numpy()\n",
    "    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
    "    labels = dataset.data.y.numpy()\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(range(np.max(edges_raw))))\n",
    "    G.add_edges_from(edges)\n",
    "    plt.subplot(111)\n",
    "    options = {\n",
    "                'node_size': 30,\n",
    "                'width': 0.2,\n",
    "    }\n",
    "    nx.draw(G, with_labels=False, node_color=labels.tolist(), cmap=plt.cm.tab10, font_weight='bold', **options)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test(data, source_features_reduced, train=True):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    pred = model(data, source_features_reduced).max(dim=1)[1]\n",
    "    if train:\n",
    "        correct += pred[train_mask].eq(data.y[train_mask]).sum().item()\n",
    "        return correct / (len(data.y[train_mask]))\n",
    "    else:\n",
    "        correct += pred[test_mask].eq(data.y[test_mask]).sum().item()\n",
    "        return correct / (len(data.y[test_mask]))\n",
    "\n",
    "\n",
    "def train(data, source_features_reduced, epochs, plot=False):\n",
    "    train_accuracies, test_accuracies = list(), list()\n",
    "    best_test_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data, source_features_reduced)\n",
    "            # print(out.shape)\n",
    "            loss = F.nll_loss(out[train_mask], data.y[train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = test(data, source_features_reduced)\n",
    "            test_acc = test(data, source_features_reduced, train=False)\n",
    "            if (test_acc > best_test_acc):\n",
    "                best_test_acc = test_acc\n",
    "                print(\"Test accuracy\", test_acc)\n",
    "                torch.save(model.state_dict(), \"/23F/228/FinalProj/Models/GCN_AMAZON_COMPUTERS.pth\")\n",
    "\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "            print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
    "                  format(epoch, loss, train_acc, test_acc))\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(train_accuracies, label=\"Train accuracy\")\n",
    "        plt.plot(test_accuracies, label=\"Validation accuracy\")\n",
    "        plt.xlabel(\"# Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "    dataset = Amazon(root='/23F/228/FinalProj/Datasets/Amazon', name='Computers')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    source_features = dataset[0].x.numpy()\n",
    "    n_in = 256\n",
    "    pca_source = PCA(n_components=n_in)\n",
    "    source_features_reduced = torch.from_numpy(pca_source.fit_transform(source_features))\n",
    "    source_features_reduced = source_features_reduced.to(device)\n",
    "    model = Net(dataset, n_in).to(device)\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    \n",
    "    #train_mask\n",
    "    train_mask = torch.zeros(13752, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(13752, dtype=torch.bool)\n",
    "    train_mask[0:10000] = True\n",
    "    test_mask[10001:] = True\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    train(data, source_features_reduced, epochs=150, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691ef1ef-436f-4341-975c-8d077ed5c15a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_computers.npz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
