{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab17366-4252-44aa-a4a5-6ef0ade40996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_photo.npz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.428347280334728\n",
      "Epoch: 000, Loss: 2.06812, Train Acc: 0.42949, Test Acc: 0.42835\n",
      "Epoch: 001, Loss: 1.93998, Train Acc: 0.42548, Test Acc: 0.41998\n",
      "Epoch: 002, Loss: 1.79917, Train Acc: 0.42792, Test Acc: 0.42312\n",
      "Test accuracy 0.4769874476987448\n",
      "Epoch: 003, Loss: 1.63302, Train Acc: 0.48945, Test Acc: 0.47699\n",
      "Test accuracy 0.5638075313807531\n",
      "Epoch: 004, Loss: 1.45661, Train Acc: 0.57661, Test Acc: 0.56381\n",
      "Test accuracy 0.6349372384937239\n",
      "Epoch: 005, Loss: 1.27411, Train Acc: 0.65365, Test Acc: 0.63494\n",
      "Test accuracy 0.6856694560669456\n",
      "Epoch: 006, Loss: 1.09697, Train Acc: 0.70647, Test Acc: 0.68567\n",
      "Test accuracy 0.7996861924686193\n",
      "Epoch: 007, Loss: 0.90137, Train Acc: 0.80896, Test Acc: 0.79969\n",
      "Test accuracy 0.819560669456067\n",
      "Epoch: 008, Loss: 0.75865, Train Acc: 0.82256, Test Acc: 0.81956\n",
      "Test accuracy 0.8315899581589958\n",
      "Epoch: 009, Loss: 0.64659, Train Acc: 0.83423, Test Acc: 0.83159\n",
      "Test accuracy 0.8849372384937239\n",
      "Epoch: 010, Loss: 0.55818, Train Acc: 0.88809, Test Acc: 0.88494\n",
      "Test accuracy 0.891213389121339\n",
      "Epoch: 011, Loss: 0.47858, Train Acc: 0.89576, Test Acc: 0.89121\n",
      "Test accuracy 0.8922594142259415\n",
      "Epoch: 012, Loss: 0.41661, Train Acc: 0.90064, Test Acc: 0.89226\n",
      "Test accuracy 0.8959205020920502\n",
      "Epoch: 013, Loss: 0.38536, Train Acc: 0.90117, Test Acc: 0.89592\n",
      "Test accuracy 0.9006276150627615\n",
      "Epoch: 014, Loss: 0.36169, Train Acc: 0.90239, Test Acc: 0.90063\n",
      "Epoch: 015, Loss: 0.33190, Train Acc: 0.90343, Test Acc: 0.90010\n",
      "Test accuracy 0.9048117154811716\n",
      "Epoch: 016, Loss: 0.31287, Train Acc: 0.90884, Test Acc: 0.90481\n",
      "Test accuracy 0.9089958158995816\n",
      "Epoch: 017, Loss: 0.30053, Train Acc: 0.91860, Test Acc: 0.90900\n",
      "Test accuracy 0.9184100418410042\n",
      "Epoch: 018, Loss: 0.28890, Train Acc: 0.92836, Test Acc: 0.91841\n",
      "Test accuracy 0.9225941422594143\n",
      "Epoch: 019, Loss: 0.27648, Train Acc: 0.93202, Test Acc: 0.92259\n",
      "Epoch: 020, Loss: 0.26404, Train Acc: 0.93394, Test Acc: 0.92155\n",
      "Epoch: 021, Loss: 0.25921, Train Acc: 0.93620, Test Acc: 0.92103\n",
      "Epoch: 022, Loss: 0.24792, Train Acc: 0.93760, Test Acc: 0.92207\n",
      "Epoch: 023, Loss: 0.24541, Train Acc: 0.94039, Test Acc: 0.92207\n",
      "Test accuracy 0.9246861924686193\n",
      "Epoch: 024, Loss: 0.23328, Train Acc: 0.94318, Test Acc: 0.92469\n",
      "Test accuracy 0.9288702928870293\n",
      "Epoch: 025, Loss: 0.22874, Train Acc: 0.94527, Test Acc: 0.92887\n",
      "Epoch: 026, Loss: 0.22158, Train Acc: 0.94736, Test Acc: 0.92782\n",
      "Test accuracy 0.930439330543933\n",
      "Epoch: 027, Loss: 0.21357, Train Acc: 0.94963, Test Acc: 0.93044\n",
      "Test accuracy 0.9314853556485355\n",
      "Epoch: 028, Loss: 0.20327, Train Acc: 0.95137, Test Acc: 0.93149\n",
      "Epoch: 029, Loss: 0.20158, Train Acc: 0.95259, Test Acc: 0.93096\n",
      "Test accuracy 0.9341004184100419\n",
      "Epoch: 030, Loss: 0.19049, Train Acc: 0.95154, Test Acc: 0.93410\n",
      "Test accuracy 0.9356694560669456\n",
      "Epoch: 031, Loss: 0.18663, Train Acc: 0.95381, Test Acc: 0.93567\n",
      "Test accuracy 0.9361924686192469\n",
      "Epoch: 032, Loss: 0.18948, Train Acc: 0.95485, Test Acc: 0.93619\n",
      "Test accuracy 0.9393305439330544\n",
      "Epoch: 033, Loss: 0.18517, Train Acc: 0.95520, Test Acc: 0.93933\n",
      "Epoch: 034, Loss: 0.17521, Train Acc: 0.95660, Test Acc: 0.93776\n",
      "Test accuracy 0.9398535564853556\n",
      "Epoch: 035, Loss: 0.16769, Train Acc: 0.95747, Test Acc: 0.93985\n",
      "Test accuracy 0.9419456066945606\n",
      "Epoch: 036, Loss: 0.15753, Train Acc: 0.95782, Test Acc: 0.94195\n",
      "Epoch: 037, Loss: 0.15867, Train Acc: 0.95695, Test Acc: 0.94195\n",
      "Test accuracy 0.9435146443514645\n",
      "Epoch: 038, Loss: 0.15926, Train Acc: 0.95573, Test Acc: 0.94351\n",
      "Test accuracy 0.944560669456067\n",
      "Epoch: 039, Loss: 0.15390, Train Acc: 0.95642, Test Acc: 0.94456\n",
      "Epoch: 040, Loss: 0.15143, Train Acc: 0.95695, Test Acc: 0.94299\n",
      "Epoch: 041, Loss: 0.15041, Train Acc: 0.95799, Test Acc: 0.94351\n",
      "Epoch: 042, Loss: 0.14528, Train Acc: 0.95904, Test Acc: 0.94351\n",
      "Epoch: 043, Loss: 0.14393, Train Acc: 0.95886, Test Acc: 0.94351\n",
      "Epoch: 044, Loss: 0.14477, Train Acc: 0.95921, Test Acc: 0.94351\n",
      "Epoch: 045, Loss: 0.14358, Train Acc: 0.95991, Test Acc: 0.94351\n",
      "Epoch: 046, Loss: 0.14225, Train Acc: 0.96026, Test Acc: 0.94195\n",
      "Epoch: 047, Loss: 0.13723, Train Acc: 0.96218, Test Acc: 0.94404\n",
      "Epoch: 048, Loss: 0.13375, Train Acc: 0.96200, Test Acc: 0.94404\n",
      "Epoch: 049, Loss: 0.13979, Train Acc: 0.96148, Test Acc: 0.94456\n",
      "Test accuracy 0.9450836820083682\n",
      "Epoch: 050, Loss: 0.13474, Train Acc: 0.96165, Test Acc: 0.94508\n",
      "Epoch: 051, Loss: 0.13312, Train Acc: 0.96113, Test Acc: 0.94351\n",
      "Epoch: 052, Loss: 0.13633, Train Acc: 0.96165, Test Acc: 0.94404\n",
      "Epoch: 053, Loss: 0.13436, Train Acc: 0.96252, Test Acc: 0.94404\n",
      "Epoch: 054, Loss: 0.13235, Train Acc: 0.96200, Test Acc: 0.94351\n",
      "Epoch: 055, Loss: 0.13384, Train Acc: 0.96252, Test Acc: 0.94247\n",
      "Epoch: 056, Loss: 0.13222, Train Acc: 0.96252, Test Acc: 0.94299\n",
      "Epoch: 057, Loss: 0.12729, Train Acc: 0.96287, Test Acc: 0.94247\n",
      "Epoch: 058, Loss: 0.12476, Train Acc: 0.96374, Test Acc: 0.94456\n",
      "Test accuracy 0.9456066945606695\n",
      "Epoch: 059, Loss: 0.13012, Train Acc: 0.96427, Test Acc: 0.94561\n",
      "Test accuracy 0.9461297071129707\n",
      "Epoch: 060, Loss: 0.12570, Train Acc: 0.96322, Test Acc: 0.94613\n",
      "Test accuracy 0.9471757322175732\n",
      "Epoch: 061, Loss: 0.12924, Train Acc: 0.96392, Test Acc: 0.94718\n",
      "Epoch: 062, Loss: 0.12238, Train Acc: 0.96514, Test Acc: 0.94561\n",
      "Epoch: 063, Loss: 0.12324, Train Acc: 0.96374, Test Acc: 0.94404\n",
      "Epoch: 064, Loss: 0.12436, Train Acc: 0.96409, Test Acc: 0.94299\n",
      "Epoch: 065, Loss: 0.12380, Train Acc: 0.96427, Test Acc: 0.94456\n",
      "Epoch: 066, Loss: 0.12382, Train Acc: 0.96409, Test Acc: 0.94718\n",
      "Epoch: 067, Loss: 0.12229, Train Acc: 0.96357, Test Acc: 0.94508\n",
      "Epoch: 068, Loss: 0.12410, Train Acc: 0.96374, Test Acc: 0.94404\n",
      "Epoch: 069, Loss: 0.12246, Train Acc: 0.96462, Test Acc: 0.94508\n",
      "Epoch: 070, Loss: 0.12381, Train Acc: 0.96584, Test Acc: 0.94665\n",
      "Epoch: 071, Loss: 0.12079, Train Acc: 0.96496, Test Acc: 0.94665\n",
      "Epoch: 072, Loss: 0.12342, Train Acc: 0.96531, Test Acc: 0.94561\n",
      "Epoch: 073, Loss: 0.12353, Train Acc: 0.96601, Test Acc: 0.94508\n",
      "Epoch: 074, Loss: 0.12351, Train Acc: 0.96427, Test Acc: 0.94508\n",
      "Epoch: 075, Loss: 0.11936, Train Acc: 0.96357, Test Acc: 0.94561\n",
      "Epoch: 076, Loss: 0.12373, Train Acc: 0.96357, Test Acc: 0.94613\n",
      "Epoch: 077, Loss: 0.12065, Train Acc: 0.96479, Test Acc: 0.94665\n",
      "Test accuracy 0.9482217573221757\n",
      "Epoch: 078, Loss: 0.12107, Train Acc: 0.96549, Test Acc: 0.94822\n",
      "Epoch: 079, Loss: 0.11857, Train Acc: 0.96496, Test Acc: 0.94822\n",
      "Epoch: 080, Loss: 0.12209, Train Acc: 0.96671, Test Acc: 0.94718\n",
      "Epoch: 081, Loss: 0.11735, Train Acc: 0.96566, Test Acc: 0.94770\n",
      "Epoch: 082, Loss: 0.11988, Train Acc: 0.96514, Test Acc: 0.94665\n",
      "Epoch: 083, Loss: 0.11932, Train Acc: 0.96549, Test Acc: 0.94770\n",
      "Epoch: 084, Loss: 0.11822, Train Acc: 0.96653, Test Acc: 0.94770\n",
      "Epoch: 085, Loss: 0.11666, Train Acc: 0.96584, Test Acc: 0.94770\n",
      "Epoch: 086, Loss: 0.11745, Train Acc: 0.96531, Test Acc: 0.94822\n",
      "Epoch: 087, Loss: 0.11777, Train Acc: 0.96566, Test Acc: 0.94718\n",
      "Epoch: 088, Loss: 0.11506, Train Acc: 0.96514, Test Acc: 0.94613\n",
      "Epoch: 089, Loss: 0.11723, Train Acc: 0.96671, Test Acc: 0.94718\n",
      "Epoch: 090, Loss: 0.11488, Train Acc: 0.96688, Test Acc: 0.94665\n",
      "Epoch: 091, Loss: 0.11359, Train Acc: 0.96723, Test Acc: 0.94718\n",
      "Epoch: 092, Loss: 0.11457, Train Acc: 0.96740, Test Acc: 0.94665\n",
      "Epoch: 093, Loss: 0.11519, Train Acc: 0.96723, Test Acc: 0.94718\n",
      "Epoch: 094, Loss: 0.11245, Train Acc: 0.96723, Test Acc: 0.94665\n",
      "Epoch: 095, Loss: 0.11532, Train Acc: 0.96671, Test Acc: 0.94508\n",
      "Epoch: 096, Loss: 0.11509, Train Acc: 0.96688, Test Acc: 0.94508\n",
      "Epoch: 097, Loss: 0.11246, Train Acc: 0.96653, Test Acc: 0.94508\n",
      "Epoch: 098, Loss: 0.11379, Train Acc: 0.96671, Test Acc: 0.94508\n",
      "Epoch: 099, Loss: 0.11276, Train Acc: 0.96653, Test Acc: 0.94613\n",
      "Epoch: 100, Loss: 0.11295, Train Acc: 0.96671, Test Acc: 0.94456\n",
      "Epoch: 101, Loss: 0.11474, Train Acc: 0.96775, Test Acc: 0.94508\n",
      "Epoch: 102, Loss: 0.11164, Train Acc: 0.96775, Test Acc: 0.94822\n",
      "Epoch: 103, Loss: 0.11127, Train Acc: 0.96828, Test Acc: 0.94822\n",
      "Epoch: 104, Loss: 0.11476, Train Acc: 0.96828, Test Acc: 0.94822\n",
      "Epoch: 105, Loss: 0.11120, Train Acc: 0.96740, Test Acc: 0.94770\n",
      "Epoch: 106, Loss: 0.11278, Train Acc: 0.96653, Test Acc: 0.94613\n",
      "Epoch: 107, Loss: 0.11053, Train Acc: 0.96653, Test Acc: 0.94665\n",
      "Epoch: 108, Loss: 0.10871, Train Acc: 0.96653, Test Acc: 0.94561\n",
      "Epoch: 109, Loss: 0.10762, Train Acc: 0.96688, Test Acc: 0.94665\n",
      "Epoch: 110, Loss: 0.10943, Train Acc: 0.96845, Test Acc: 0.94613\n",
      "Epoch: 111, Loss: 0.10959, Train Acc: 0.96758, Test Acc: 0.94456\n",
      "Epoch: 112, Loss: 0.11040, Train Acc: 0.96688, Test Acc: 0.94456\n",
      "Epoch: 113, Loss: 0.11083, Train Acc: 0.96758, Test Acc: 0.94665\n",
      "Epoch: 114, Loss: 0.10998, Train Acc: 0.96967, Test Acc: 0.94665\n",
      "Epoch: 115, Loss: 0.11035, Train Acc: 0.97002, Test Acc: 0.94456\n",
      "Epoch: 116, Loss: 0.11131, Train Acc: 0.96793, Test Acc: 0.94508\n",
      "Epoch: 117, Loss: 0.10747, Train Acc: 0.96723, Test Acc: 0.94404\n",
      "Epoch: 118, Loss: 0.10715, Train Acc: 0.96758, Test Acc: 0.94404\n",
      "Epoch: 119, Loss: 0.10797, Train Acc: 0.96828, Test Acc: 0.94561\n",
      "Epoch: 120, Loss: 0.11017, Train Acc: 0.96775, Test Acc: 0.94508\n",
      "Epoch: 121, Loss: 0.10497, Train Acc: 0.96671, Test Acc: 0.94456\n",
      "Epoch: 122, Loss: 0.10986, Train Acc: 0.96810, Test Acc: 0.94561\n",
      "Epoch: 123, Loss: 0.10824, Train Acc: 0.97037, Test Acc: 0.94822\n",
      "Epoch: 124, Loss: 0.10775, Train Acc: 0.97037, Test Acc: 0.94718\n",
      "Test accuracy 0.9492677824267782\n",
      "Epoch: 125, Loss: 0.10940, Train Acc: 0.96932, Test Acc: 0.94927\n",
      "Epoch: 126, Loss: 0.10947, Train Acc: 0.96740, Test Acc: 0.94718\n",
      "Epoch: 127, Loss: 0.10963, Train Acc: 0.96862, Test Acc: 0.94665\n",
      "Epoch: 128, Loss: 0.10596, Train Acc: 0.96967, Test Acc: 0.94770\n",
      "Epoch: 129, Loss: 0.11000, Train Acc: 0.96880, Test Acc: 0.94665\n",
      "Epoch: 130, Loss: 0.10538, Train Acc: 0.96862, Test Acc: 0.94561\n",
      "Epoch: 131, Loss: 0.10519, Train Acc: 0.96897, Test Acc: 0.94613\n",
      "Epoch: 132, Loss: 0.10288, Train Acc: 0.97019, Test Acc: 0.94822\n",
      "Test accuracy 0.9497907949790795\n",
      "Epoch: 133, Loss: 0.10711, Train Acc: 0.96950, Test Acc: 0.94979\n",
      "Epoch: 134, Loss: 0.10504, Train Acc: 0.97019, Test Acc: 0.94770\n",
      "Epoch: 135, Loss: 0.10276, Train Acc: 0.96897, Test Acc: 0.94613\n",
      "Epoch: 136, Loss: 0.10583, Train Acc: 0.96740, Test Acc: 0.94613\n",
      "Epoch: 137, Loss: 0.10671, Train Acc: 0.96967, Test Acc: 0.94718\n",
      "Epoch: 138, Loss: 0.10608, Train Acc: 0.96880, Test Acc: 0.94561\n",
      "Epoch: 139, Loss: 0.10240, Train Acc: 0.96897, Test Acc: 0.94665\n",
      "Epoch: 140, Loss: 0.10427, Train Acc: 0.96967, Test Acc: 0.94718\n",
      "Epoch: 141, Loss: 0.10374, Train Acc: 0.96967, Test Acc: 0.94770\n",
      "Epoch: 142, Loss: 0.10218, Train Acc: 0.97037, Test Acc: 0.94665\n",
      "Epoch: 143, Loss: 0.10136, Train Acc: 0.96950, Test Acc: 0.94927\n",
      "Epoch: 144, Loss: 0.10317, Train Acc: 0.97019, Test Acc: 0.94822\n",
      "Epoch: 145, Loss: 0.10111, Train Acc: 0.96950, Test Acc: 0.94718\n",
      "Epoch: 146, Loss: 0.10107, Train Acc: 0.97002, Test Acc: 0.94927\n",
      "Epoch: 147, Loss: 0.10412, Train Acc: 0.97107, Test Acc: 0.94979\n",
      "Test accuracy 0.9503138075313807\n",
      "Epoch: 148, Loss: 0.10305, Train Acc: 0.97054, Test Acc: 0.95031\n",
      "Epoch: 149, Loss: 0.09972, Train Acc: 0.96915, Test Acc: 0.94822\n"
     ]
    }
   ],
   "source": [
    "#if using colab, do !pip install torch_geometric before running this file\n",
    "from torch_geometric.datasets import Amazon\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Step 1: Add self-loops\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Multiply with weights\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3: Calculate the normalization\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4: Propagate the embeddings to the next layer\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x,\n",
    "                              norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset, n_in):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(n_in, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        self.out = torch.nn.Linear(64, dataset.num_classes)\n",
    "        self.hook = self.conv2.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.intermediate_output = output\n",
    "\n",
    "    def forward(self, data, source_features_reduced):\n",
    "        x, edge_index = source_features_reduced, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def plot_dataset(dataset):\n",
    "    edges_raw = dataset.data.edge_index.numpy()\n",
    "    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
    "    labels = dataset.data.y.numpy()\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(range(np.max(edges_raw))))\n",
    "    G.add_edges_from(edges)\n",
    "    plt.subplot(111)\n",
    "    options = {\n",
    "                'node_size': 30,\n",
    "                'width': 0.2,\n",
    "    }\n",
    "    nx.draw(G, with_labels=False, node_color=labels.tolist(), cmap=plt.cm.tab10, font_weight='bold', **options)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test(data, source_features_reduced, train=True):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    pred = model(data, source_features_reduced).max(dim=1)[1]\n",
    "    if train:\n",
    "        correct += pred[train_mask].eq(data.y[train_mask]).sum().item()\n",
    "        return correct / (len(data.y[train_mask]))\n",
    "    else:\n",
    "        correct += pred[test_mask].eq(data.y[test_mask]).sum().item()\n",
    "        return correct / (len(data.y[test_mask]))\n",
    "\n",
    "\n",
    "def train(data, source_features_reduced, epochs, plot=False):\n",
    "    train_accuracies, test_accuracies = list(), list()\n",
    "    best_test_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data, source_features_reduced)\n",
    "            # print(out.shape)\n",
    "            loss = F.nll_loss(out[train_mask], data.y[train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = test(data, source_features_reduced)\n",
    "            test_acc = test(data, source_features_reduced, train=False)\n",
    "            if (test_acc > best_test_acc):\n",
    "                best_test_acc = test_acc\n",
    "                print(\"Test accuracy\", test_acc)\n",
    "                torch.save(model.state_dict(), \"/23F/228/FinalProj/Models/GCN_AMAZON_PHOTO.pth\")\n",
    "\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "            print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
    "                  format(epoch, loss, train_acc, test_acc))\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(train_accuracies, label=\"Train accuracy\")\n",
    "        plt.plot(test_accuracies, label=\"Validation accuracy\")\n",
    "        plt.xlabel(\"# Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "    dataset = Amazon(root='/23F/228/FinalProj/Datasets/Amazon', name='Photo')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    source_features = dataset[0].x.numpy()\n",
    "    n_in = 256\n",
    "    pca_source = PCA(n_components=n_in)\n",
    "    source_features_reduced = torch.from_numpy(pca_source.fit_transform(source_features))\n",
    "    source_features_reduced = source_features_reduced.to(device)\n",
    "    model = Net(dataset, n_in).to(device)\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    \n",
    "    #train_mask\n",
    "    train_mask = torch.zeros(7650, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(7650, dtype=torch.bool)\n",
    "    train_mask[0:5737] = True\n",
    "    test_mask[5738:] = True\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    train(data, source_features_reduced, epochs=150, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ef1ef-436f-4341-975c-8d077ed5c15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
